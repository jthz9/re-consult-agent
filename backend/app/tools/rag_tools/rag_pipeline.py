import os
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from app.tools.rag_tools.loaders.document_loader import DocumentLoader
from app.tools.rag_tools.splitters.text_splitter import TextSplitter
from app.tools.rag_tools.utils.logger import get_logger

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

logger = get_logger(__name__)

class RAGPipeline:
    """RAG(Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸"""
    
    def __init__(
        self,
        model_name: str = "gpt-4o",
        primary_embedding_model: str = "text-embedding-3-small",
        backup_embedding_model: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
        persist_directory: str = "./app/tools/rag_tools/vectorstores/data",
        collection_name: str = "knrec_faq",
        embedding_type: str = "auto"  # ì¶”ê°€: 'openai', 'huggingface', 'auto'
    ):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            model_name: LLM ëª¨ë¸ ì´ë¦„
            primary_embedding_model: ê¸°ë³¸ ì„ë² ë”© ëª¨ë¸ (OpenAI)
            backup_embedding_model: ë°±ì—… ì„ë² ë”© ëª¨ë¸ (HuggingFace)
            persist_directory: ë²¡í„° ì €ì¥ì†Œ ë””ë ‰í† ë¦¬
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            embedding_type: 'openai', 'huggingface', 'auto'
        """
        self.model_name = model_name
        self.primary_embedding_model = primary_embedding_model
        self.backup_embedding_model = backup_embedding_model
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.embedding_type = embedding_type
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model_name=model_name,
            temperature=0.3  # ë” ì¼ê´€ë˜ê³  êµ¬ì¡°í™”ëœ ì‘ë‹µì„ ìœ„í•´ ë‚®ì¶¤
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (embedding_typeì— ë”°ë¼ ê°•ì œ ì§€ì •)
        self.embeddings = self._initialize_embeddings()
        
        # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (Chroma ì‚¬ìš©)
        self.vectorstore = self._initialize_vectorstore()
        
        # ë¬¸ì„œ ë¡œë” ì´ˆê¸°í™”
        self.document_loader = DocumentLoader()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        self.prompt_template = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì¬ìƒì—ë„ˆì§€ ì „ë¬¸ AI ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹µë³€ ì‘ì„± ì›ì¹™:
- ë‚´ìš©ì— í¬í•¨ëœ êµ¬ì²´ì ì¸ ì •ë³´, ìˆ˜ì¹˜, ì ˆì°¨ ë“±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€ ì‘ì„±
- ì œëª©ì˜ ë¶ˆì™„ì „í•œ ë¶€ë¶„ì€ ë¬´ì‹œí•˜ê³ , ë‚´ìš©ì˜ ì™„ì „í•œ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€
- í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì „ë‹¬
- ë§ˆí¬ë‹¤ìš´, êµµì€ ê¸€ì”¨, íŠ¹ìˆ˜ ê¸°í˜¸ ë“±ì€ ì‚¬ìš©í•˜ì§€ ë§ê³  ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ë‹µë³€
- ë¬¸ì¥ì´ ì¤‘ê°„ì— ëŠê¸°ê±°ë‚˜ ë¶ˆì™„ì „í•œ ê²½ìš°, ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ì™„ì„±ëœ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€

ì•„ë˜ [ì´ì „ ëŒ€í™”]ì™€ [ì»¨í…ìŠ¤íŠ¸]ì˜ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
íŠ¹íˆ ë‚´ìš©(content)ì— í¬í•¨ëœ ì™„ì „í•œ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.

[ì´ì „ ëŒ€í™”]
{history}

[ì»¨í…ìŠ¤íŠ¸]
{context}

[ì§ˆë¬¸]
{question}

ë‹µë³€:
""")
        
        # RAG ì²´ì¸ ì„¤ì •
        self.chain = (
            {"context": self.vectorstore.as_retriever(), "question": RunnablePassthrough()}
            | self.prompt_template
            | self.llm
            | StrOutputParser()
        )
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
        self.text_splitter = TextSplitter(
            chunk_size=500,  # ë” ì‘ì€ ì²­í¬ í¬ê¸°ë¡œ ë¬¸ì¥ ì¤‘ê°„ ì ˆë‹¨ ë°©ì§€
            chunk_overlap=100  # ì ì ˆí•œ ì¤‘ë³µ ìœ ì§€
        )
    
    def _initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (embedding_typeì— ë”°ë¼ ê°•ì œ ì§€ì •)"""
        base_data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))), "data", "vectorstores")
        if self.embedding_type == "openai":
            logger.info("OpenAI ì„ë² ë”© ëª¨ë¸ì„ ê°•ì œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.persist_directory = os.path.join(base_data_dir, "openai")
            return OpenAIEmbeddings(
                model=self.primary_embedding_model,
                openai_api_key=os.getenv("OPENAI_API_KEY")
            )
        elif self.embedding_type == "huggingface":
            logger.info("HuggingFace ì„ë² ë”© ëª¨ë¸ì„ ê°•ì œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.persist_directory = os.path.join(base_data_dir, "huggingface")
            return HuggingFaceEmbeddings(
                model_name=self.backup_embedding_model,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        else:
            # ê¸°ì¡´ auto fallback ë¡œì§
            try:
                if os.getenv("OPENAI_API_KEY"):
                    logger.info("OpenAI ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    self.persist_directory = os.path.join(base_data_dir, "openai")
                    return OpenAIEmbeddings(
                        model=self.primary_embedding_model,
                        openai_api_key=os.getenv("OPENAI_API_KEY")
                    )
                else:
                    logger.warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°±ì—… ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    raise Exception("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                logger.info("ë°±ì—… ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.persist_directory = os.path.join(base_data_dir, "huggingface")
                return HuggingFaceEmbeddings(
                    model_name=self.backup_embedding_model,
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
    
    def _initialize_vectorstore(self):
        """Chroma ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”"""
        try:
            # ê¸°ì¡´ Chroma ì»¬ë ‰ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
            if os.path.exists(self.persist_directory):
                logger.info(f"ê¸°ì¡´ Chroma ì»¬ë ‰ì…˜ì„ ë¡œë“œí•©ë‹ˆë‹¤: {self.persist_directory}")
                return Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings,
                    collection_name=self.collection_name
                )
            else:
                logger.info(f"ìƒˆë¡œìš´ Chroma ì»¬ë ‰ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤: {self.persist_directory}")
                # ë¹ˆ Chroma ì»¬ë ‰ì…˜ ìƒì„±
                return Chroma.from_texts(["ì´ˆê¸°í™”"], self.embeddings, persist_directory=self.persist_directory, collection_name=self.collection_name)
        except Exception as e:
            logger.warning(f"Chroma ì»¬ë ‰ì…˜ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {str(e)}")
            return Chroma.from_texts(["ì´ˆê¸°í™”"], self.embeddings, persist_directory=self.persist_directory, collection_name=self.collection_name)
    
    def get_embedding_model_info(self) -> Dict[str, str]:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì„ë² ë”© ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        model_type = "OpenAI" if isinstance(self.embeddings, OpenAIEmbeddings) else "HuggingFace"
        model_name = self.primary_embedding_model if model_type == "OpenAI" else self.backup_embedding_model
        
        return {
            "type": model_type,
            "name": model_name,
            "status": "primary" if model_type == "OpenAI" else "backup"
        }
    
    def load_documents(self, documents: List[Dict[str, Any]]) -> None:
        """ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
        
        Args:
            documents: ë¡œë“œí•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë¬¸ì„œ ë¡œë“œ
            loaded_docs = self.document_loader.load_documents(documents)
            
            # Chromaì— ë¬¸ì„œ ì¶”ê°€
            self.vectorstore.add_documents(loaded_docs)
            
            # Chroma ì €ì¥
            self.vectorstore.persist()
            
            logger.info(f"ì´ {len(loaded_docs)}ê°œ ë¬¸ì„œê°€ Chroma ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def get_relevant_documents(self, query: str, k: int = 3) -> List[Document]:
        """ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            return self.vectorstore.similarity_search(query, k=k)
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def query(self, query: str, history: Optional[str] = None) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)"""
        docs = self.vectorstore.similarity_search_with_score(query, k=5)
        filtered_docs = []
        for doc, score in docs:
            similarity_score = 1 / (1 + score)
            if similarity_score >= 0.3:
                filtered_docs.append(doc)
        if not filtered_docs:
            logger.warning(f"ì¿¼ë¦¬ '{query}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜, ì¬ìƒì—ë„ˆì§€ ê´€ë ¨ ì§ˆë¬¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                "documents": []
            }
        context_parts = []
        seen_content = set()
        for doc in filtered_docs:
            content_hash = hash(doc.page_content.strip())
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                cleaned_content = doc.page_content.strip()
                if cleaned_content and not cleaned_content.endswith('...'):
                    sentences = cleaned_content.split('.')
                    valid_sentences = []
                    for sentence in sentences:
                        sentence = sentence.strip()
                        if sentence and not sentence.endswith('...') and len(sentence) > 10:
                            valid_sentences.append(sentence)
                    if valid_sentences:
                        cleaned_content = '. '.join(valid_sentences) + '.'
                        context_parts.append(cleaned_content)
        context = "\n\n---\n\n".join(context_parts)
        # historyê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ
        if history is None:
            history = ""
        # ë‹µë³€ ìƒì„± (í”„ë¡¬í”„íŠ¸ì— history ì¶”ê°€)
        response = self.prompt_template.invoke({
            "history": history,
            "context": context,
            "question": query
        })
        response = self.llm.invoke(response)
        response = StrOutputParser().invoke(response)
        return {
            "answer": self._post_process_response(response),
            "documents": [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in filtered_docs
            ]
        }
    
    def _post_process_response(self, response: str) -> str:
        """ì‘ë‹µ í›„ì²˜ë¦¬ - ê°€ë…ì„± í–¥ìƒ
        
        Args:
            response: ì›ë³¸ ì‘ë‹µ
            
        Returns:
            í›„ì²˜ë¦¬ëœ ì‘ë‹µ
        """
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        response = response.strip()
        
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒì„ 2ê°œë¡œ)
        import re
        response = re.sub(r'\n{3,}', '\n\n', response)
        
        # ë¬¸ì¥ ë ì •ë¦¬
        response = re.sub(r'\s+([.!?])', r'\1', response)
        
        # ë§ˆí¬ë‹¤ìš´ íŒ¨í„´ ì œê±° (**í…ìŠ¤íŠ¸** â†’ í…ìŠ¤íŠ¸)
        response = re.sub(r'\*\*(.*?)\*\*', r'\1', response)
        
        # ê¸°íƒ€ ë§ˆí¬ë‹¤ìš´ íŒ¨í„´ ì œê±°
        response = re.sub(r'\*(.*?)\*', r'\1', response)  # *í…ìŠ¤íŠ¸* â†’ í…ìŠ¤íŠ¸
        response = re.sub(r'`(.*?)`', r'\1', response)    # `í…ìŠ¤íŠ¸` â†’ í…ìŠ¤íŠ¸
        
        # ë§ì¤„ì„í‘œë¡œ ëë‚˜ëŠ” ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œê±°
        lines = response.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            # ë§ì¤„ì„í‘œë¡œ ëë‚˜ê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
            if line and not line.endswith('...') and len(line) > 10:
                cleaned_lines.append(line)
        
        response = '\n'.join(cleaned_lines)
        
        # ì°¸ê³  ë¬¸ì„œ ì„¹ì…˜ì—ì„œ ë¶ˆì™„ì „í•œ ë‚´ìš© ì œê±°
        if 'ğŸ“‹ ì°¸ê³  ë¬¸ì„œ:' in response:
            parts = response.split('ğŸ“‹ ì°¸ê³  ë¬¸ì„œ:')
            main_content = parts[0].strip()
            response = main_content
        
        return response
    
    def load_directory(
        self,
        directory_path: str,
        glob_pattern: str = "**/*.*",
        persist: bool = True
    ) -> None:
        """ë””ë ‰í† ë¦¬ ë‚´ ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        
        Args:
            directory_path: ë””ë ‰í† ë¦¬ ê²½ë¡œ
            glob_pattern: íŒŒì¼ íŒ¨í„´
            persist: ì €ì¥ ì—¬ë¶€
        """
        # ë¬¸ì„œ ë¡œë“œ
        loaded_docs = self.document_loader.load_directory(directory_path, glob_pattern)
        
        # ë¬¸ì„œ ë¶„í• 
        split_docs = self.text_splitter.split_documents(loaded_docs)
        
        # Chromaì— ë¬¸ì„œ ì¶”ê°€
        self.vectorstore.add_documents(split_docs)
        
        if persist:
            # Chroma ì €ì¥
            self.vectorstore.persist()
    
    def rebuild_vectorstore(self, documents: List[Dict[str, Any]]) -> None:
        """ë²¡í„° ì €ì¥ì†Œ ì¬êµ¬ì„± (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œìš´ ë¶„í•  ë°©ì‹ìœ¼ë¡œ ì¬êµ¬ì„±)
        
        Args:
            documents: ì¬êµ¬ì„±í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ì‚­ì œ
            import shutil
            if os.path.exists(self.persist_directory):
                shutil.rmtree(self.persist_directory)
                logger.info(f"ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤: {self.persist_directory}")
            
            # ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
            self.vectorstore = self._initialize_vectorstore()
            
            # ë¬¸ì„œ ì¬ë¡œë“œ
            self.load_documents(documents)
            
            logger.info("ë²¡í„° ì €ì¥ì†Œê°€ ìƒˆë¡œìš´ ë¶„í•  ë°©ì‹ìœ¼ë¡œ ì¬êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ë²¡í„° ì €ì¥ì†Œ ì¬êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise 